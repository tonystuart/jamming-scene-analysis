Jamming Scene Analysis converts an image of a natural or synthetic scene into a musical composition.

It is based on Jamming with Color (aka Jamming) as demonstrated at the Seattle Mini Maker Faire, September 19th and 20th, 2015.

Jamming Scene Analysis is based on the assumption that scenes contain repeating and nested patterns that can be converted into pleasing musical compositions by finding the hidden model that relates the two.

Some of these models may be relatively straightforward, like dividing the scene into a grid of rows and columns (or columns and rows) and converting the average color of each square in the grid into a note, chord or drum beat. Similar adjacent squares can be combined to produce variations in timing.

A more complex model may consist of sampling the scene at different resolutions for different parts of the composition. For example, a low resolution sampling for the introduction, a medium resolution sampling for the chorus, a high resolution sampling for each verse and a derivative of the intro for the ending.

From here, one can add rules for creating bridges, instrumental solos and other interesting things... and sequential translation of the grid squares is only one option, with other possibilities including non-linear patterns or algorithms that look for interesting objects, colors and relationships in the scene.
